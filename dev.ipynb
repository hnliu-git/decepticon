{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from project.data.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = RaceDataProcessor()\n",
    "processor.process_data(\"RACE\", \"LON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All of the following steps should be done from command line. They are here to a quick & dirty heuristic to work with jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Hparams = namedtuple(\"Hparams\", [\"data_path\", \"batch_size\", \"num_workers\", \"special_tokens\", \"pretrained_model\"])\n",
    "hparams = Hparams(\n",
    "    data_path = \"LON\",\n",
    "    batch_size = 16,\n",
    "    num_workers = 6,\n",
    "    special_tokens = [\"[CON]\", \"[QUE]\", \"[ANS]\", \"[DIS]\"],\n",
    "    pretrained_model = \"bert-base-cased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customed_collate_fn(batch, tokenizer):\n",
    "    \"\"\"\"\"\"\n",
    "    import torch\n",
    "    articles = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    distractors = []\n",
    "\n",
    "    for item in batch:\n",
    "        articles.append(\" \".join([\"<answer>\", item[\"answer\"], \"<context>\", item[\"article\"]]))\n",
    "        questions.append(item[\"question\"])\n",
    "    articles = tokenizer(articles, padding=True, \n",
    "                                       truncation=True, \n",
    "                                       return_tensors=\"pt\", \n",
    "                                       pad_to_max_length=True, \n",
    "                                       max_length=512)\n",
    "    questions = tokenizer(questions, padding=True, \n",
    "                                       truncation=True, \n",
    "                                       return_tensors=\"pt\", \n",
    "                                       pad_to_max_length=True, \n",
    "                                       max_length=512)\n",
    "    articles['input_ids'] = torch.squeeze(articles['input_ids'])\n",
    "    articles['attention_mask'] = torch.squeeze(articles['attention_mask'])\n",
    "    questions['input_ids'] = torch.squeeze(questions['input_ids'])\n",
    "    questions['attention_mask'] = torch.squeeze(questions['attention_mask'])\n",
    "    \n",
    "    return (articles, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and setup \n",
    "data_module = RaceDataModule(hparams, customed_collate_fn)\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data loaders:\n",
    "trainloader = data_module.train_dataloader()\n",
    "valloader = data_module.val_dataloader()\n",
    "testloader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con cac dit meCon cac dit meCon cac dit meCon cac dit me\n",
      "Con cac dit me\n",
      "\n",
      "\n",
      "\n",
      "Con cac dit me\n",
      "Con cac dit me\n",
      "Con cac dit me\n",
      "Con cac dit me\n",
      "Con cac dit me\n",
      "Con cac dit me\n",
      "Con cac dit me\n",
      "Con cac dit me\n"
     ]
    }
   ],
   "source": [
    "cac = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[ 101,  133, 2590,  ...,    0,    0,    0],\n",
       "          [ 101,  133, 2590,  ...,    0,    0,    0],\n",
       "          [ 101,  133, 2590,  ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [ 101,  133, 2590,  ...,    0,    0,    0],\n",
       "          [ 101,  133, 2590,  ...,    0,    0,    0],\n",
       "          [ 101,  133, 2590,  ...,    0,    0,    0]]),\n",
       "  'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " {'input_ids': tensor([[  101,  1327,  1169,  1195,  3858,  1121,  1142,  5885,   136,   102,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0],\n",
       "          [  101,  1327,  1132,  1113,  4688,  1113, 15638,  1105, 15757,  1340,\n",
       "             136,   102,     0,     0,     0,     0,     0,     0],\n",
       "          [  101,  1109,  4459,  1494,  1103,  5885,  3114,  1366,  1110,   168,\n",
       "             119,   102,     0,     0,     0,     0,     0,     0],\n",
       "          [  101,  1130,  8122,  4708,   112,   188,  4893,   117,  1117,  6122,\n",
       "            6486,  1158,  1108,  8449,  1272,   168,   119,   102],\n",
       "          [  101, 15075,  1103,  3342,   117,  7302,  1108,  2265,  1272,   168,\n",
       "             119,   102,     0,     0,     0,     0,     0,     0],\n",
       "          [  101,  1327,  1110,  1103,  3087,  2871,  1164,   136,   102,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0],\n",
       "          [  101,  1109,  1299,  1397,  1106,  2132,  1105,  2639,  4061,  1146,\n",
       "            1272,   168,   119,   102,     0,     0,     0,     0],\n",
       "          [  101,  5979,  1104,  1103,  1378,  1110, 24819,  1942,  2276,  2452,\n",
       "            1106,  1103,  5885,   136,   102,     0,     0,     0],\n",
       "          [  101,  1109,  5885,  1110,  1211,  1930,  1678,  1149,  1104,   168,\n",
       "             119,   102,     0,     0,     0,     0,     0,     0],\n",
       "          [  101,  2009,  1108,  1103,  1148,  3676,  3753,  1165,  1119,  2242,\n",
       "            1103,  1402,   136,   102,     0,     0,     0,     0],\n",
       "          [  101,  1109,  2006,  5885,  1110,  8663,  1113,  1103,  2398,  1206,\n",
       "             168,   119,   102,     0,     0,     0,     0,     0],\n",
       "          [  101,  2009,  1674,  1103,  3676,  1301,  1106,  1103,  2298,   136,\n",
       "            2279,   168,   119,   102,     0,     0,     0,     0],\n",
       "          [  101,  1135,  1882,  1115,  1103,  1202, 19030,  1116, 11176,  1103,\n",
       "            1447,  1114,  1147, 21315,  1106,   168,   119,   102],\n",
       "          [  101,  1327, 11788,  1169,  1195,  3858,  1121,  1103,  5885,   136,\n",
       "             102,     0,     0,     0,     0,     0,     0,     0],\n",
       "          [  101,  1109,  1590,  1108,  5925,  1272,   168,   119,   102,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0],\n",
       "          [  101,  1109,  2351, 12246,  1106,  1587,  1366,   168,   119,   102,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       "  'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32102, 512)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [\"cac\", \"[CAC]\"]})\n",
    "model = AutoModel.from_pretrained(\"t5-small\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['cac', '[CAC]']})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Cac1] acw fwef efwefwef[Cac2] sgs ssvsd sfsf</s>'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(\"[Cac1] acw fwef efwefwef [Cac2] sgs ssvsd sfsf \")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def concac(a, b):\n",
    "    return a + b\n",
    "cailon = partial(concac, b=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cailon(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
