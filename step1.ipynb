{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surface-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gersa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "# neptune\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from time import sleep\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.nn import functional as F\n",
    "from transformers import  T5ForConditionalGeneration, T5Config, get_linear_schedule_with_warmup, AdamW\n",
    "from project.data.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "native-insertion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8504, 0.9204],\n",
       "        [0.2023, 0.7988],\n",
       "        [0.6397, 0.8530]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand((3,2))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "considered-launch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(t.transpose(0,1), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-magnitude",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "working-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Processing: skip\n"
     ]
    }
   ],
   "source": [
    "run_preprocessing = False\n",
    "# No need to rerun the processing\n",
    "\n",
    "if run_preprocessing:\n",
    "    processor = RaceDataProcessor()\n",
    "    processor.process_data(\"RACE/\", \"Processed_New/\")\n",
    "else:\n",
    "    print(\"Data Processing: skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "annoying-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_path =  \"D:\\Github/decepticon/Processed_New\"\n",
    "        self.batch_size =  16\n",
    "        self.num_workers =  0\n",
    "        self.pretrained_model = \"t5-small\"\n",
    "        self.tokenizer_len = 0\n",
    "        self.padding_id = 0\n",
    "    def to_dict(self):\n",
    "        out = dict()\n",
    "        out[\"data_path\"] = self.data_path\n",
    "        out[\"batch_size\"] = self.batch_size\n",
    "        out[\"num_workers\"] = self.num_workers\n",
    "        out[\"pretrained_model\"] = self.pretrained_model\n",
    "        out[\"tokenizer_len\"] = self.tokenizer_len\n",
    "        out[\"weight_decay\"] = self.weight_decay\n",
    "        out[\"learning_rate\"] = self.learning_rate\n",
    "        out[\"padding_id\"] = self.padding_id\n",
    "        return out\n",
    "\n",
    "hparams=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "approximate-punishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32102\n",
      "SETUP: Training Dataset\n",
      "SETUP: Validation Dataset\n",
      "SETUP: Test Dataset\n"
     ]
    }
   ],
   "source": [
    "data_module = RaceDataModule(hparams)\n",
    "## Colab is weird, so  I have to put it here\n",
    "#data_module.tokenizer.add_special_tokens({'additional_special_tokens': ['<answer>', '<context>']})\n",
    "print(len(data_module.tokenizer))\n",
    "hparams.tokenizer_len = len(data_module.tokenizer)\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "guided-clause",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32102"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "altered-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "C:\\Users\\gersa\\.conda\\envs\\network\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:50: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "241.969   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLATE based of t5-small\n",
      "COLLATE based of t5-small\n",
      "COLLATE based of t5-small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69508b6cf572478583dec97a9ae62537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLATE based of t5-small\n",
      "COLLATE based of t5-small\n",
      "COLLATE based of t5-small\n",
      "COLLATE based of t5-small\n",
      "COLLATE based of t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gersa\\.conda\\envs\\network\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5FinetuneForRACE(hparams.to_dict())\n",
    "trainer = Trainer(accumulate_grad_batches=4)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "african-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FinetuneForRACE(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FinetuneForRACE, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams = hparams\n",
    "        if self.hparams.pretrained_model in [\"t5-base\",\"t5-small\":\n",
    "            config = T5Config(decoder_start_token_id=0)\n",
    "            self.model = T5ForConditionalGeneration(config).from_pretrained(self.hparams.pretrained_model)\n",
    "            self.model.resize_token_embeddings(self.hparams.tokenizer_len)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    def mask_label_padding(self, labels):\n",
    "        MASK_ID = -100\n",
    "        labels[labels == self.hparams.padding_id] = MASK_ID \n",
    "        return labels\n",
    " \n",
    "    def forward(self, ids, mask, labels):\n",
    "        return self.model(input_ids = ids, attention_mask = mask, labels = labels)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x[\"input_ids\"], x[\"attention_mask\"], self.mask_label_padding(y[\"input_ids\"]))\n",
    "        loss = output.loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x[\"input_ids\"], x[\"attention_mask\"], self.mask_label_padding(y[\"input_ids\"]))\n",
    "        loss = output[0]\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x[\"input_ids\"], x[\"attention_mask\"], self.mask_label_padding(y[\"input_ids\"]))\n",
    "        loss = output[0]\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [{'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                                         'weight_decay': 0.01}, \n",
    "                                        {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                         'weight_decay': 0.0}]\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr = self.hparams.learning_rate)\n",
    "#         scheduler = get_linear_schedule_with_warmup(\n",
    "#             optimizer,\n",
    "#             num_warmup_steps=0,\n",
    "#             # Default value in run_glue.py\n",
    "#             num_training_steps=self.hparams.num_training_steps)\n",
    " \n",
    "        return [optimizer]#, [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suited-riverside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([32100,   432,     8,   756,     5, 32101,   932,   305,    19,  4318,\n",
    "          4351,    31,     7,  1430,     5,    86,   685,     6,    34,    19,\n",
    "          1086,   718,  7508,    31,     7,  1430,   250,    34,    19,     3,\n",
    "          4894,  9443,    41,     3,    61,    57,  5234,     5,    37,  5216,\n",
    "            13,  7508,    31,     7,  1430,    65,     3,     9,   307,   892,\n",
    "             5,    94,    19,   243,    24,     8,  3994,   639,    45,     8,\n",
    "         10282, 17733,  3397,    16,  1473,     5,   461,  7508,    31,     7,\n",
    "          1430,     6,  4318,  5234,     3,  1544,     3,     9,   534,   773,\n",
    "            13,  6605,  4340,     5,    94,    19,  2303,    28,     3,     9,\n",
    "          8384,    11,  3353,    28, 15431, 11388,    41,     3,   137,  4351,\n",
    "           333,    12,     3,  1544,    34,   182,   231,     5,   461,    48,\n",
    "           239,     6,   502,   278,    31,    17,    43,    12,   281,    12,\n",
    "           496,     5,  1698,   384,    28,     3,     9,  4940,  5168,     7,\n",
    "            95,  1450,   443,   102,    18,  6489,  5692,    41,     3,   137,\n",
    "            37,  5692,     7,    33,  1086,   386,  2602,    10,  1001,     6,\n",
    "          1131,    11,  1692,     5,  1589,  5024,    21,     8,  2353,     6,\n",
    "          1131,  5024,    21,     8,  2039,    11,  1692,  5024,    21,     8,\n",
    "           520,     5,  4318,  1362,   857,    24,     8,  5692,     7,    56,\n",
    "           830,    70,  5234,   207,  5851,    11,   428,   135, 11578,    11,\n",
    "           579,     5,     1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tamil-douglas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<answer> All the above.<context> May 5 is Japanese Children's Day. In fact, it is usually called Boy's Day because it is mainly celebrated ( ) by boys. The celebration of Boy's Day has a long history. It is said that the festival comes from the Dragon Boat Festival in China. On Boy's Day, Japanese boys eat a special kind of rice cake. It is covered with a leaf and filled with bean paste ( ). Children love to eat it very much. On this day, children don't have to go to school. Each family with a boy hangs up huge carp-shaped flag ( ). The flags are usually three colors: black, red and blue. Black stands for the father, red stands for the mother and blue stands for the son. Japanese parents believe that the flags will bring their boys good luck and give them courage and power.</s>\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data_module.tokenizer.decode(t) for t in i[\"articles\"][\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dynamic-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in data_module.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "retained-nerve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "joint-final",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-2cbc24866934>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "obvious-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLATE based of t5-base\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f6955750bcbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Github\\decepticon\\project\\data\\data.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained_model\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"t5-base\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m                 \u001b[0marticles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"<answer>\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"answer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"<context>\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"article\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m                 \u001b[0mquestions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0mdistractors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep_token\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"distractors\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "data_module.train_dataloader().collate_fn(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-particle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
