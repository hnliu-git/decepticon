{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "behind-deficit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gersa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "# neptune\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from time import sleep\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "from transformers import  T5ForConditionalGeneration, T5Config, get_linear_schedule_with_warmup, AdamW\n",
    "from project.data.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-force",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daily-armor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Processing: skip\n"
     ]
    }
   ],
   "source": [
    "run_preprocessing = False\n",
    "# No need to rerun the processing\n",
    "\n",
    "if run_preprocessing:\n",
    "    processor = RaceDataProcessor()\n",
    "    processor.process_data(\"RACE/\", \"Processed_New/\")\n",
    "else:\n",
    "    print(\"Data Processing: skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compatible-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    data_path =  \"Processed_New/\"\n",
    "    batch_size =  16\n",
    "    num_workers =  0\n",
    "    pretrained_model = \"t5-base\"\n",
    "    tokenizer_len = 0\n",
    "    weights_decay = 1e-5\n",
    "    learning_rate =1e-5\n",
    "\n",
    "hparams=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "favorite-reserve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETUP: Training Dataset\n",
      "SETUP: Validation Dataset\n",
      "SETUP: Test Dataset\n"
     ]
    }
   ],
   "source": [
    "data_module = RaceDataModule(hparams)\n",
    "## Colab is weird, so  I have to put it here\n",
    "#data_module.tokenizer.add_special_tokens({'additional_special_tokens': ['<answer>', '<context>']})\n",
    "hparams.tokenizer_len = len(data_module.tokenizer)\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5FinetuneForRACE(hparams)\n",
    "\n",
    "trainer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optical-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FinetuneForRACE(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FinetuneForRACE, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams = hparams\n",
    "        if self.hparams.pretrained_model == \"t5-base\":\n",
    "            config = T5Config(decoder_start_token_id=0)\n",
    "            self.model = T5ForConditionalGeneration(config).from_pretrained(self.hparams.pretrained_model)\n",
    "            self.model.resize_token_embeddings(self.hparams.tokenizer_len)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    " \n",
    "    def forward(self, ids, mask, y_ids, lm_labels):\n",
    "        return model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, decoder_input_ids, lm_labels = batch\n",
    "        y_hat = self(input_ids, attention_mask, decoder_input_ids, lm_labels)\n",
    "        loss = F.cross_entropy(y_hat[0], y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('valid_loss', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [{'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                                         'weight_decay': 0.01}, \n",
    "                                        {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                                         'weight_decay': 0.0}]\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr = self.hparams.learning_rate)\n",
    "#         scheduler = get_linear_schedule_with_warmup(\n",
    "#             optimizer,\n",
    "#             num_warmup_steps=0,\n",
    "#             # Default value in run_glue.py\n",
    "#             num_training_steps=self.hparams.num_training_steps)\n",
    " \n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "partial-daniel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([32100,   432,     8,   756,     5, 32101,   932,   305,    19,  4318,\n",
    "          4351,    31,     7,  1430,     5,    86,   685,     6,    34,    19,\n",
    "          1086,   718,  7508,    31,     7,  1430,   250,    34,    19,     3,\n",
    "          4894,  9443,    41,     3,    61,    57,  5234,     5,    37,  5216,\n",
    "            13,  7508,    31,     7,  1430,    65,     3,     9,   307,   892,\n",
    "             5,    94,    19,   243,    24,     8,  3994,   639,    45,     8,\n",
    "         10282, 17733,  3397,    16,  1473,     5,   461,  7508,    31,     7,\n",
    "          1430,     6,  4318,  5234,     3,  1544,     3,     9,   534,   773,\n",
    "            13,  6605,  4340,     5,    94,    19,  2303,    28,     3,     9,\n",
    "          8384,    11,  3353,    28, 15431, 11388,    41,     3,   137,  4351,\n",
    "           333,    12,     3,  1544,    34,   182,   231,     5,   461,    48,\n",
    "           239,     6,   502,   278,    31,    17,    43,    12,   281,    12,\n",
    "           496,     5,  1698,   384,    28,     3,     9,  4940,  5168,     7,\n",
    "            95,  1450,   443,   102,    18,  6489,  5692,    41,     3,   137,\n",
    "            37,  5692,     7,    33,  1086,   386,  2602,    10,  1001,     6,\n",
    "          1131,    11,  1692,     5,  1589,  5024,    21,     8,  2353,     6,\n",
    "          1131,  5024,    21,     8,  2039,    11,  1692,  5024,    21,     8,\n",
    "           520,     5,  4318,  1362,   857,    24,     8,  5692,     7,    56,\n",
    "           830,    70,  5234,   207,  5851,    11,   428,   135, 11578,    11,\n",
    "           579,     5,     1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "blessed-detail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<answer> All the above.<context> May 5 is Japanese Children's Day. In fact, it is usually called Boy's Day because it is mainly celebrated ( ) by boys. The celebration of Boy's Day has a long history. It is said that the festival comes from the Dragon Boat Festival in China. On Boy's Day, Japanese boys eat a special kind of rice cake. It is covered with a leaf and filled with bean paste ( ). Children love to eat it very much. On this day, children don't have to go to school. Each family with a boy hangs up huge carp-shaped flag ( ). The flags are usually three colors: black, red and blue. Black stands for the father, red stands for the mother and blue stands for the son. Japanese parents believe that the flags will bring their boys good luck and give them courage and power.</s>\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data_module.tokenizer.decode(t) for t in i[\"articles\"][\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "technical-philip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLATE based of t5-base\n",
      "{'articles': {'input_ids': tensor([[32100,   432,     8,   756,     5, 32101,   932,   305,    19,  4318,\n",
      "          4351,    31,     7,  1430,     5,    86,   685,     6,    34,    19,\n",
      "          1086,   718,  7508,    31,     7,  1430,   250,    34,    19,     3,\n",
      "          4894,  9443,    41,     3,    61,    57,  5234,     5,    37,  5216,\n",
      "            13,  7508,    31,     7,  1430,    65,     3,     9,   307,   892,\n",
      "             5,    94,    19,   243,    24,     8,  3994,   639,    45,     8,\n",
      "         10282, 17733,  3397,    16,  1473,     5,   461,  7508,    31,     7,\n",
      "          1430,     6,  4318,  5234,     3,  1544,     3,     9,   534,   773,\n",
      "            13,  6605,  4340,     5,    94,    19,  2303,    28,     3,     9,\n",
      "          8384,    11,  3353,    28, 15431, 11388,    41,     3,   137,  4351,\n",
      "           333,    12,     3,  1544,    34,   182,   231,     5,   461,    48,\n",
      "           239,     6,   502,   278,    31,    17,    43,    12,   281,    12,\n",
      "           496,     5,  1698,   384,    28,     3,     9,  4940,  5168,     7,\n",
      "            95,  1450,   443,   102,    18,  6489,  5692,    41,     3,   137,\n",
      "            37,  5692,     7,    33,  1086,   386,  2602,    10,  1001,     6,\n",
      "          1131,    11,  1692,     5,  1589,  5024,    21,     8,  2353,     6,\n",
      "          1131,  5024,    21,     8,  2039,    11,  1692,  5024,    21,     8,\n",
      "           520,     5,  4318,  1362,   857,    24,     8,  5692,     7,    56,\n",
      "           830,    70,  5234,   207,  5851,    11,   428,   135, 11578,    11,\n",
      "           579,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}, 'questions': {'input_ids': tensor([[4318, 1362,  857,   24,    8,  443,  102,   18, 6489, 5692,    7,   56,\n",
      "            3,  834,    3,    5,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}}\n"
     ]
    }
   ],
   "source": [
    "for i in data_module.train_dataloader():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "republican-coral",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-2cbc24866934>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "swedish-victoria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLATE based of t5-base\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f6955750bcbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Github\\decepticon\\project\\data\\data.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained_model\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"t5-base\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m                 \u001b[0marticles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"<answer>\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"answer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"<context>\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"article\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m                 \u001b[0mquestions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0mdistractors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep_token\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"distractors\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "data_module.train_dataloader().collate_fn(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-tribute",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
