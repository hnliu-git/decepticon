# Python Import:
from argparse import ArgumentParser

# Pytorch Import:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import Dataset, DataLoader

# Pytorch Lightning Import:
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger

# HuggingFace Import:
from transformers import AutoTokenizer, AutoModel


class RaceModule(pl.LightningModule):
    """"""
    @staticmethod
    def add_model_specific_args(parent_parser):
        """ Args:
                source_size (int): The expected number of features in the input.
                target_size (int): The expected number of sequence features.
                hidden_size (int): The number of features in the hidden state.
                num_layers (int): Number of recurrent layers.
                bidirectional (boolean): whether to use bidirectional model.
                dropout (float): dropout probability.
                lr (float): Learning rate.
        """
        parser = ArgumentParser(parents=[parent_parser], add_help=False)
        parser.add_argument("--d_model", type=int, default=256)
        parser.add_argument("--nhead", type=int, default=1)
        parser.add_argument("--num_layers", type=bool, default=False)
        parser.add_argument("--learning_rate", type=float, default=1e-3)
        parser.add_argument("--pretrained_model", type=str, default="prajjwal1/bert-tiny",
                            help="Pretrained model.")
        return parser

    def __init__(self, hparams):
        super().__init__()

        # Encoder:
        self.encoder = AutoModel.from_pretrained(hparams.pretrained_model)
        for param in self.encoder.parameters():
            param.requires_grad = False

        # Decoder:
        self.embedding = self.encoder.embeddings
        self.decoder_layer = nn.TransformerDecoderLayer(d_model=hparams.d_model, nhead=hparams.nhead)
        self.decoder = nn.TransformerDecoder(self.decoder_layer, hparams.num_layers)#, nn.LayerNorm(hparams.d_model, 1e-12))

        # Head:
        self.head = nn.Linear(hparams.d_model, self.encoder.embeddings.word_embeddings.num_embeddings)

    def generate_tgt_mask(self, size):
        """"""
        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(self, input, target):
        """ Args:
                hidden (num_layers, batch, hidden_size): States of the GRU.
                pred_len (int): Length of predicted sequence.
                target (batch, seq_len, target_size): Target sequence. If None,
                    the output sequence is generated by feeding the output
                    of the previous timestep (teacher_forcing has to be False).
                teacher_forcing (float): Probability to apply teacher forcing.

            Returns:
                outputs (batch, seq_len, target_size): Tensor of log-probabilities
                    of words in the target language.
                hidden of shape (1, batch_size, hidden_size): New states of the GRU.
        """
        # Encode:
        encode = self.encoder(**input).last_hidden_state.permute((1, 0, 2))

        # Decode:
        tgt = self.embedding(target["input_ids"]).permute((1, 0, 2))
        tgt_mask = self.generate_tgt_mask(tgt.shape[0]).to(tgt.device)
        tgt_key_padding_mask = target["attention_mask"] == 0
        memory_key_padding_mask = input["attention_mask"] == 0
        decode = self.decoder(tgt, encode, tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask,
                              memory_key_padding_mask=memory_key_padding_mask)

        # Head:
        output = F.softmax(self.head(decode), dim=-1)

        return output, encode, decode, tgt

    def configure_optimizers(self):
        optimizer = Adam(self.parameters(), lr=self.hparams.learning_rate)
        scheduler = ReduceLROnPlateau(optimizer, mode="min", factor=1e-1, patience=2, verbose=True)
        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}

    def training_step(self, batch, batch_idx):
        x, y = batch
        _, h = self.encode(x)
        y_hat, _ = self(h, None, y, 1.0)
        loss = F.mse_loss(y_hat, y)
        return {"loss": loss}

    def training_epoch_end(self, outputs):
        loss = torch.stack([x["loss"] for x in outputs]).mean()
        self.log("loss", loss, logger=True)

    def validation_step(self, batch, batch_idx):
        x, y = batch
        _, h = self.encode(x)
        y_hat, _ = self(h, None, y, 0.0)
        val_loss = F.mse_loss(y_hat, y)
        return {'val_loss': val_loss}

    def validation_epoch_end(self, outputs):
        val_loss = torch.stack([x["val_loss"] for x in outputs]).mean()
        self.log("val_loss", val_loss, prog_bar=True, logger=True)


if __name__ == "__main__":
    # Argument parser:
    parser = ArgumentParser()
    parser = RaceDataModule.add_model_specific_args(parser)
    parser = RaceModule.add_model_specific_args(parser)
    parser = pl.Trainer.add_argparse_args(parser)
    args = parser.parse_args()

    # Model & data module:
    fx_dm = RaceDataModule(args)
    fx_model = RaceModule(args)

    # Callbacks:
    checkpoint = ModelCheckpoint(
        filepath="./checkpoint/fx-{epoch:02d}-{val_loss:.7f}",
        monitor="val_loss"
    )

    # Logger:
    logger = TensorBoardLogger('logs/')

    # Trainer:
    trainer = pl.Trainer.from_argparse_args(
        args,
        checkpoint_callback=checkpoint,
        logger=logger
    )
    trainer.fit(fx_model, fx_dm)

    fx_infer = RaceModule.load_from_checkpoint(checkpoint.best_model_path)
    fx_infer.eval()
    print(fx_infer)

