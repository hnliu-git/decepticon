{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "retained-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "# neptune\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from time import sleep\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.nn import functional as F\n",
    "from models.t5 import *\n",
    "from data.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "careful-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5FinetuneForRACE.load_from_checkpoint(\"D:/Github/decepticon/project/checkpoints/0_02.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "varied-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_path =  \"D:\\Github/decepticon/Processed_New\"\n",
    "        self.batch_size =  16\n",
    "        self.num_workers =  0\n",
    "        self.pretrained_model = \"t5-small\"\n",
    "        self.tokenizer_len = 32102 ## manual\n",
    "        self.padding_id = 0\n",
    "    def to_dict(self):\n",
    "        out = dict()\n",
    "        out[\"data_path\"] = self.data_path\n",
    "        out[\"batch_size\"] = self.batch_size\n",
    "        out[\"num_workers\"] = self.num_workers\n",
    "        out[\"pretrained_model\"] = self.pretrained_model\n",
    "        out[\"tokenizer_len\"] = self.tokenizer_len\n",
    "        out[\"padding_id\"] = self.padding_id\n",
    "        return out\n",
    "\n",
    "hparams=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rental-expert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32102\n",
      "SETUP: Training Dataset\n",
      "SETUP: Validation Dataset\n",
      "SETUP: Test Dataset\n"
     ]
    }
   ],
   "source": [
    "data_module = RaceDataModule(hparams)\n",
    "print(len(data_module.tokenizer))\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "surface-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(model, content):\n",
    "    model.eval()        \n",
    "    #output = model.model.generate(input_ids=content[\"input_ids\"])\n",
    "    output = model.model.generate(input_ids = content[\"input_ids\"], \n",
    "                                  num_beams = 5,\n",
    "                                  max_lengths = 50,\n",
    "                                  early_stopping = True)\n",
    "    decode = data_module.tokenizer.decode(output[0], skip_special_tokens= True, clean_up_tokenization_spaces = True)\n",
    "    content = [data_module.tokenizer.decode(i) for i in content[\"input_ids\"]]\n",
    "    return decode, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acknowledged-mainstream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "GROUND TRUTH: The Sherman Antitrust Act _.\n",
      "GENERATED: The Sherman Antitrust Act _.\n",
      "ANSWER: sought to eliminate monopolies in favor of competition in the market-place\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Why are ECAs important now?\n",
      "GENERATED: What does the passage mainly talk about?\n",
      "ANSWER: They prepare the students for their future.\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Which of the following should be avoided when taking selfies?\n",
      "GENERATED: Which of the following can make the photo appealing?\n",
      "ANSWER: A busy background.\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: What can be the subject of this passage?\n",
      "GENERATED: Which of the following is TRUE according to the passage?\n",
      "ANSWER: People across the world get married in different ways and for different reasons.\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Why didn't the fish eat our night crawlers?\n",
      "GENERATED: Which of the following is TRUE according to the passage?\n",
      "ANSWER: The reason was not clear.\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Why did the author feel nervous when collecting the afternoon mails? _\n",
      "GENERATED: Why did the author ask her husband to set up a time each week to talk about her\n",
      "ANSWER: She could feel the pressure of the huge debt.\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Which of the following majors can get a job easily?\n",
      "GENERATED: According to Allmen-Vinniage, students who do find jobs started preparing\n",
      "ANSWER: accounting\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: The old saying referred to in the passage tells us that _.\n",
      "GENERATED: We can infer from the passage that _.\n",
      "ANSWER: eating apples regularly does lots of good to our health\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: We can learn from the text that _.\n",
      "GENERATED: We can learn from the passage that _.\n",
      "ANSWER: different people can bear different amounts of stress\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: The passage is most probably entitled \" _ \".\n",
      "GENERATED: What is the best title for the passage?\n",
      "ANSWER: American Superstitions\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Which of the following is the hest title for the passage?\n",
      "GENERATED: What would be the best title for the passage?\n",
      "ANSWER: Practice Makes a Man a Better Speechmaker\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x, y in data_module.val_dataloader():\n",
    "    output = generate_question(model, x)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"GROUND TRUTH: %s\" %data_module.tokenizer.decode(y[\"input_ids\"][0], skip_special_tokens= True, clean_up_tokenization_spaces = True))\n",
    "    print(\"GENERATED: %s\"  %\"\".join(output[0]))\n",
    "    print(\"ANSWER: %s\" % \"\".join(output[1]).split(\"<context>\")[0].replace(\"<answer>\", \"\").lstrip())\n",
    "    i += 1\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-fellow",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
