{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aggregate-horse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gersa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "# neptune\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from time import sleep\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.nn import functional as F\n",
    "from models.t5 import *\n",
    "from data.data import *\n",
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "downtown-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5FinetuneForRACE.load_from_checkpoint(\"D:/Github/decepticon/project/checkpoints/0_02.ckpt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stuck-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_path =  \"D:\\Github/decepticon/Processed_New\"\n",
    "        self.batch_size =  16\n",
    "        self.num_workers =  0\n",
    "        self.pretrained_model = \"t5-small\"\n",
    "        self.tokenizer_len = 32102 ## manual\n",
    "        self.padding_id = 0\n",
    "    def to_dict(self):\n",
    "        out = dict()\n",
    "        out[\"data_path\"] = self.data_path\n",
    "        out[\"batch_size\"] = self.batch_size\n",
    "        out[\"num_workers\"] = self.num_workers\n",
    "        out[\"pretrained_model\"] = self.pretrained_model\n",
    "        out[\"tokenizer_len\"] = self.tokenizer_len\n",
    "        out[\"padding_id\"] = self.padding_id\n",
    "        return out\n",
    "\n",
    "hparams=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beginning-cancer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETUP: Training Dataset\n",
      "SETUP: Validation Dataset\n",
      "SETUP: Test Dataset\n"
     ]
    }
   ],
   "source": [
    "data_module = RaceDataModule(hparams)\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incoming-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(content):\n",
    "    model.eval()        \n",
    "    generated = model.model.generate(input_ids = x[\"input_ids\"].to(device), \n",
    "                                     num_beams = 6,\n",
    "                                     num_return_sequences = 5,\n",
    "                                     max_length = 50,\n",
    "                                     no_repeat_ngram_size = 2,\n",
    "                                     early_stopping = True)\n",
    "\n",
    "    output = []\n",
    "    for sequence in generated[:5]:\n",
    "        output.append(data_module.tokenizer.decode(sequence, skip_special_tokens= True, clean_up_tokenization_spaces = True))\n",
    "    content = data_module.tokenizer.decode(content[\"input_ids\"][0], skip_special_tokens= False, clean_up_tokenization_spaces = True)\n",
    "    return output, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "applicable-evanescence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "GROUND TRUTH: The Sherman Antitrust Act _.\n",
      "GENERATED 0: The Sherman Antitrust Act _.\n",
      "GENERATED 1: According to the passage, the early tycoons _.\n",
      "GENERATED 2: It can be inferred from the passage that the Sherman Antitrust Act _.\n",
      "GENERATED 3: The first tycoons built successful businesses because they _.\n",
      "GENERATED 4: The Sherman Antitrust Act was passed because it _.\n",
      "ANSWER: sought to eliminate monopolies in favor of competition in the market-place\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Why are ECAs important now?\n",
      "GENERATED 0: What does the passage mainly talk about?\n",
      "GENERATED 1: Which of the following is TRUE about ECAs?\n",
      "GENERATED 2: What does the passage mainly tell us about ECAs?\n",
      "GENERATED 3: Which of the following is NOT true about ECAs?\n",
      "GENERATED 4: What's the main idea of the passage?\n",
      "ANSWER: They prepare the students for their future.\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Which of the following should be avoided when taking selfies?\n",
      "GENERATED 0: What's the main problem with selfies?\n",
      "GENERATED 1: What's the main problem with taking selfies?\n",
      "GENERATED 2: Which of the following can make the photo appealing?\n",
      "GENERATED 3: According to the passage, which of the following can make the photo appealing?\n",
      "GENERATED 4: Which of the following can make a selfie appealing?\n",
      "ANSWER: A busy background.\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: What can be the subject of this passage?\n",
      "GENERATED 0: Which of the following is TRUE according to the passage?\n",
      "GENERATED 1: Which of the following statements is TRUE according to the passage?\n",
      "GENERATED 2: Which of the following is NOT true according to the passage?\n",
      "GENERATED 3: Which of the following is true according to the passage?\n",
      "GENERATED 4: According to the passage, which of the following is TRUE?\n",
      "ANSWER: People across the world get married in different ways and for different reasons.\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Why didn't the fish eat our night crawlers?\n",
      "GENERATED 0: Which of the following is TRUE according to the passage?\n",
      "GENERATED 1: Which of the following is NOT true according to the passage?\n",
      "GENERATED 2: Which of the following is true according to the passage?\n",
      "GENERATED 3: Which of the following is TRUE according to the text?\n",
      "GENERATED 4: Which of the following statements is TRUE?\n",
      "ANSWER: The reason was not clear.\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Why did the author feel nervous when collecting the afternoon mails? _\n",
      "GENERATED 0: Why did the author ask her husband to set up a time to talk about her finances?\n",
      "GENERATED 1: Why did the author ask her husband to set up a time each week?\n",
      "GENERATED 2: How did the author feel when she saw the mails?\n",
      "GENERATED 3: What did the author think of her husband?\n",
      "GENERATED 4: Which of the following is true about the author?\n",
      "ANSWER: She could feel the pressure of the huge debt.\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Which of the following majors can get a job easily?\n",
      "GENERATED 0: According to Allmen-Vinniage, students have majored in _.\n",
      "GENERATED 1: According to Allmen-Vinniage, students who do find jobs started preparing two years ago have majored in _.\n",
      "GENERATED 2: According to Allmen-Vinniage, students who find jobs started preparing two years ago have majored in _.\n",
      "GENERATED 3: According to Allmen-Vinniage, students have majored in one of the few fields that are still hot.\n",
      "GENERATED 4: According to Allmen-Vinniage, students have majored in one of the few fields that are still hot.\n",
      "ANSWER: accounting\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: The old saying referred to in the passage tells us that _.\n",
      "GENERATED 0: It can be inferred from the passage that _.\n",
      "GENERATED 1: We can infer from the passage that _.\n",
      "GENERATED 2: We can learn from the passage that _.\n",
      "GENERATED 3: The writer thinks that _.\n",
      "GENERATED 4: According to the passage, we can infer that _.\n",
      "ANSWER: eating apples regularly does lots of good to our health\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: We can learn from the text that _.\n",
      "GENERATED 0: We can learn from the passage that _.\n",
      "GENERATED 1: We can infer from the passage that _.\n",
      "GENERATED 2: It can be inferred from the passage that_.\n",
      "GENERATED 3: According to the passage,we know that _.\n",
      "GENERATED 4: From the passage we know that _.\n",
      "ANSWER: different people can bear different amounts of stress\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: The passage is most probably entitled \" _ \".\n",
      "GENERATED 0: What is the best title for the passage?\n",
      "GENERATED 1: What would be the best title for the passage?\n",
      "GENERATED 2: What's the best title for the passage?\n",
      "GENERATED 3: What's the best title of the passage?\n",
      "GENERATED 4: What is the best title for this passage?\n",
      "ANSWER: American Superstitions\n",
      "-----------------------------------------\n",
      "GROUND TRUTH: Which of the following is the hest title for the passage?\n",
      "GENERATED 0: What would be the best title for the passage?\n",
      "GENERATED 1: Which of the following is the best title for the passage?\n",
      "GENERATED 2: What's the best title for the passage?\n",
      "GENERATED 3: What is the best title for the passage?\n",
      "GENERATED 4: The best title for the passage is _.\n",
      "ANSWER: Practice Makes a Man a Better Speechmaker\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for x, y in data_module.val_dataloader():\n",
    "    output = generate_question(x)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"GROUND TRUTH: %s\" %data_module.tokenizer.decode(y[\"input_ids\"][0], skip_special_tokens= True, clean_up_tokenization_spaces = True))\n",
    "    for i, q in enumerate(output[0]):\n",
    "        print(\"GENERATED %s: %s\"  %(i, \"\".join(q)))\n",
    "    print(\"ANSWER: %s\" % \"\".join(output[1]).split(\"<context>\")[0].replace(\"<answer>\", \"\").lstrip())\n",
    "    j += 1\n",
    "    if j > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-classification",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
